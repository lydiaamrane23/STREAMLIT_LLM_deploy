# -*- coding: utf-8 -*-
"""iza.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DUpk6RfDQ4zCQSVzzRWXRRcAr24jxpo2
"""

import numpy as np


# Commented out IPython magic to ensure Python compatibility.
# %%writefile i.pdf
# Emmanuel Macron (/ɛmanɥɛl makʁɔ̃/e Écouter), né le 21 décembre 1977 à Amiens (Somme), est un haut fonctionnaire et homme d'État français. Il est président de la République française depuis le 14 mai 2017.
# Sorti de l'École nationale d'administration en 2004, il devient inspecteur des finances.

# Commented out IPython magic to ensure Python compatibility.
# %%writefile i2.pdf
# Jacques Chirac (/ʒak ʃiʁak/b Écouter), né le 29 novembre 1932 dans le 5e arrondissement de Paris et mort le 26 septembre 2019 dans le 6e arrondissement de la même ville, est un haut fonctionnaire et homme d'État français.
# 
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile i1.pdf
# 
# Élisabeth Borne, née le 18 avril 1961 à Paris, est une haute fonctionnaire et femme d'État française.
# 
# Polytechnicienne, ingénieure générale des ponts et chaussées, préfète de la région Poitou-Charentes de 2013 à 2014 puis directrice de cabinet de Ségolène Royal au ministère de l'Écologie de 2014 à 2015, elle est présidente de la Régie autonome des transports parisiens (RATP) de 2015 à 2017.

from langchain.document_loaders import DirectoryLoader,TextLoader

loader = DirectoryLoader('./',glob="**/*.pdf")
data = loader.load()





from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=0)
docs = text_splitter.split_documents(data)




from langchain.embeddings.openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(openai_api_key =  "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" )
print(embeddings)



#auth_client_secret=auth_config,
import weaviate
client = weaviate.Client(
    embedded_options=weaviate.EmbeddedOptions(),
    additional_headers={
        "X-OpenAI-Api-Key": "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"

    }
)

from langchain.vectorstores import Weaviate



# define input structure
client.schema.delete_all()
client.schema.get()
schema = {
    "classes": [
        {
            "class": "Chatbot",
            "description": "Documents for chatbot",
            "vectorizer": "text2vec-openai",
            "moduleConfig": {"text2vec-openai": {"model": "ada", "type": "text"}},
            "properties": [
                {
                    "dataType": ["text"],
                    "description": "The content of the paragraph",
                    "moduleConfig": {
                        "text2vec-openai": {
                            "skip": False,
                            "vectorizePropertyName": False,
                        }
                    },
                    "name": "content",
                },
            ],
        },
    ]
}

client.schema.create(schema)

vectorstore = Weaviate(client, "Chatbot", "content", attributes=["source"])

# load text into the vectorstore
text_meta_pair = [(doc.page_content, doc.metadata) for doc in docs]
#print(text_meta_pair)
texts, meta = list(zip(*text_meta_pair))
print(meta)



vectorstore.add_texts(texts, meta)

query = "qui est Emmanuel?"

# retrieve text related to the query
docs = vectorstore.similarity_search(query)

docs



import streamlit as st
import pandas as pd

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI

# define chain
chain = load_qa_chain(
    OpenAI(openai_api_key ="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",temperature=0),
    chain_type="stuff")

# create answer
chain.run(input_documents=docs, question=query)



# step 1:  le but de bout de code : interface + RECUPERATION DE LA QUESTION

#%%writefile my_app1.py
import streamlit as st
import numpy as np
from snowflake.snowpark.context import get_active_session

# Write directly to the app
st.title("ASK THE DOC APP")
st.sidebar.header("Specify ATTRIBUT")
selected_number = st.sidebar.text_input("Mission Number")
selected_type = st.sidebar.text_input("Mission type")
selected_location = st.sidebar.text_input("Mission location")
selected_client = st.sidebar.text_input("Client")
selected_category = st.sidebar.selectbox("Mission category", ["All","Contrats","CR","Cahier de charge","Devis/offres","Factures"])
with st.form('my_form'):
  query_text = st.text_input('Enter your question :', '')
  submitted = st.form_submit_button('Submit')

print(query_text)

!pip install -U weaviate-client

## step 2: interagir avec weaviate en lui posant la question et avoir un retour
# Temporary key for livestream only
openai_key = "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"

import weaviate



## replace EMBEDDEDOPTION BY THIS WHEN i Wwill have information
#auth_config = weaviate.AuthApiKey(api_key="test")
#url = "http://weaviate:8080",  # Replace with your endpoint
#auth_client_secret=auth_config,

client = weaviate.Client(
    embedded_options=weaviate.EmbeddedOptions(),
    additional_headers={
        "X-OpenAI-Api-Key": openai_key
    }
)

import json
def jsonprint(data_in):
    print(json.dumps(data_in, indent=2))

jsonprint(client.get_meta())

#créer une nouvelle définition de classe ici. Nous allons mettre en place une classe appelée "Question" avec :
#Un "vectoriseur" -> qui convertira les données en vecteurs, qui représentent le sens,
#Un module "génératif" -> qui nous permettra d'utiliser des LLM avec nos données,

client.schema.exists("Question")

### en cas ou on a une classe qui porte le meme nom que la notre on supprime
if client.schema.exists("Question"):
    client.schema.delete_class("Question")

class_definition = {
    "class": "Question",
    "vectorizer": "text2vec-openai",
   # "vectorIndexConfig": {
    #    "distance": "cosine",
  #  },
    #"moduleConfig": {
     #   "generative-cohere": {}
    #},
   "properties": [
        {
            "name": "question",
            "dataType": ["text"]
        },
        {
            "name": "answer",
            "dataType": ["text"]
        },
    ],
}

client.schema.create_class(class_definition)

##verifier que la classe a ete bien crée
jsonprint(client.schema.get("Question"))

response = (
        client.query.get("Question", ["question", "answer"])
        .with_limit(2)
        .do()
    )

jsonprint(response)

!pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-mpnet-base-v2')

def search_for(my_question):
    vector_question = model.encode(my_question).tolist()
    #print(vector_question)
    response = (
        client.query\
        .get("Question", ["question", "answer"])\
        .with_near_vector({\
            "vector": vector_question\
        })\


        .with_limit(3)\
        .do()\
    )

    return response

### ICI on remplace le string par query_text

search_for("how is ..... ? ")

#### ICI LA REPONSE SERA UN JSON

!pip install langchain

### step3: envoyer les document selectionnés au LLM  => reponse a la question
from langchain.llms import OpenAI

def generate_response(input_text):
  llm = OpenAI(temperature=0.7, openai_api_key=openai_key)
  st.info(llm(input_text))









